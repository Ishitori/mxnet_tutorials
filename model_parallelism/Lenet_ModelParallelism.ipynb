{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import print_function\n",
    "import numpy as np\n",
    "import mxnet as mx\n",
    "from mxnet import nd, autograd, gluon\n",
    "from mxnet.gluon.block import HybridBlock\n",
    "from mxnet.gluon import nn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load MNIST training and validation data from gluon.data.vision"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 64\n",
    "\n",
    "# Make channel the first dimension. Normalize the input.\n",
    "def transform(data, label):\n",
    "    return nd.transpose(data.astype(np.float32), (2,0,1))/255, label.astype(np.float32)\n",
    "\n",
    "# Load training data from gluon.data.vision.MNIST\n",
    "train_data = gluon.data.DataLoader(gluon.data.vision.MNIST(train=True, transform=transform),\n",
    "                                      batch_size, shuffle=True)\n",
    "\n",
    "# Load validation data from gluon.data.vision.MNIST\n",
    "test_data = gluon.data.DataLoader(gluon.data.vision.MNIST(train=False, transform=transform),\n",
    "                                     batch_size, shuffle=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We break LeNet into two parts:\n",
    "- `LenetPart1` contains the convolutional layers and will be placed in the first GPU\n",
    "- `LenetPart2` contains the fully connected layers and will be placed in the second GPU  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LenetPart1(HybridBlock):\n",
    "    def __init__(self, **kwargs):\n",
    "        super(LenetPart1, self).__init__(**kwargs)\n",
    "        with self.name_scope():\n",
    "            self.layers = nn.HybridSequential(prefix='')\n",
    "            with self.layers.name_scope():\n",
    "                self.layers.add(nn.Conv2D(channels=20, kernel_size=5, activation='relu'))\n",
    "                self.layers.add(nn.MaxPool2D(pool_size=2, strides=2))\n",
    "                self.layers.add(nn.Conv2D(channels=50, kernel_size=5, activation='relu'))\n",
    "                self.layers.add(nn.MaxPool2D(pool_size=2, strides=2))\n",
    "                self.layers.add(gluon.nn.Flatten())\n",
    "    \n",
    "    def hybrid_forward(self, F, x):\n",
    "        x = self.layers(x)\n",
    "        return x\n",
    "    \n",
    "\n",
    "class LenetPart2(HybridBlock):\n",
    "    def __init__(self, classes=10, **kwargs):\n",
    "        super(LenetPart2, self).__init__(**kwargs)\n",
    "        with self.name_scope():\n",
    "            self.layers = nn.HybridSequential(prefix='')\n",
    "            with self.layers.name_scope():\n",
    "                self.layers.add(nn.Dense(512, activation=\"relu\"))\n",
    "                self.layers.add(nn.Dense(classes))\n",
    "    \n",
    "    def hybrid_forward(self, F, x):\n",
    "        x = self.layers(x)\n",
    "        return x    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Initialize `LenetPart1` on the first GPU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "ctx1 = mx.gpu(0)\n",
    "net1 = LenetPart1()\n",
    "net1.collect_params().initialize(mx.init.Xavier(magnitude=2.24), ctx=ctx1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Initialize `LenetPart2` on the second GPU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "net2 = LenetPart2()\n",
    "ctx2 = mx.gpu(1)\n",
    "net2.collect_params().initialize(mx.init.Xavier(magnitude=2.24), ctx=ctx2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We create two trainers for the two parts of the network since trainer can only handle parameters from one device."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer1 = gluon.Trainer(net1.collect_params(), 'sgd', {'learning_rate': .1})\n",
    "trainer2 = gluon.Trainer(net2.collect_params(), 'sgd', {'learning_rate': .1})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We'll use the SoftmaxCrossEntropyLoss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "softmax_cross_entropy = gluon.loss.SoftmaxCrossEntropyLoss()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When evaluating the network, we need to pass the data through the first part of the network, capture the output and pass it through the second part of the network to get the final output.\n",
    "\n",
    "Data is loaded in the context where the first part of the network resides. Label is loaded in the context where the second part of the network resides."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_accuracy(data_iterator, net1, net2):\n",
    "    acc = mx.metric.Accuracy()\n",
    "    for i, (data, label) in enumerate(data_iterator):\n",
    "        # Load data in the gpu that has the first part of the network\n",
    "        data  = data.as_in_context(ctx1)\n",
    "        # Load label in the gpu that has the last part of the network\n",
    "        label = label.as_in_context(ctx2)\n",
    "        \n",
    "        # Capture output of the first part of the network,\n",
    "        # pass it though the second part of the network to get the final output.\n",
    "        output1 = net1(data)\n",
    "        input2  = output1.as_in_context(ctx2)\n",
    "        output  = net2(input2)\n",
    "        \n",
    "        # Usual accuracy calculation\n",
    "        predictions = nd.argmax(output, axis=1)\n",
    "        acc.update(preds=predictions, labels=label)\n",
    "    return acc.get()[1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "While training, data is passed through the two parts of network just like how it was done above in `evaluate_accuracy`. For the backward pass, MXNet takes care of copying the gradients from the second gpu to the first."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch[0] Moving loss: 2.293158\n",
      "Batch[100] Moving loss: 1.369541\n",
      "Batch[200] Moving loss: 0.641024\n",
      "Batch[300] Moving loss: 0.339356\n",
      "Batch[400] Moving loss: 0.205750\n",
      "Batch[500] Moving loss: 0.153345\n",
      "Batch[600] Moving loss: 0.111642\n",
      "Batch[700] Moving loss: 0.106622\n",
      "Batch[800] Moving loss: 0.088289\n",
      "Batch[900] Moving loss: 0.084093\n",
      "Epoch 0. Loss: 0.0827988462407, Train_acc 0.976866666667, Test_acc 0.9786\n"
     ]
    }
   ],
   "source": [
    "epochs = 1\n",
    "\n",
    "for e in range(epochs):\n",
    "    for i, (data, label) in enumerate(train_data):\n",
    "        \n",
    "        data  = data.as_in_context(ctx1)\n",
    "        label = label.as_in_context(ctx2)\n",
    "        \n",
    "        with autograd.record():\n",
    "            output1 = net1(data)\n",
    "            input2  = output1.as_in_context(ctx2)\n",
    "            output  = net2(input2)\n",
    "            \n",
    "            loss = softmax_cross_entropy(output, label)\n",
    "            \n",
    "        loss.backward()\n",
    "        \n",
    "        trainer1.step(batch_size)\n",
    "        trainer2.step(batch_size)\n",
    "\n",
    "        curr_loss = nd.mean(loss).asscalar()\n",
    "\n",
    "        smoothing_constant = .01\n",
    "        moving_loss = (curr_loss if ((i == 0) and (e == 0))\n",
    "                       else (1 - smoothing_constant) * moving_loss + smoothing_constant * curr_loss)\n",
    "\n",
    "        if(i % 100 == 0):\n",
    "            print(\"Batch[%d] Moving loss: %f\" % (i, moving_loss))\n",
    "        \n",
    "    test_accuracy = evaluate_accuracy(test_data, net1, net2)\n",
    "    train_accuracy = evaluate_accuracy(train_data, net1, net2)\n",
    "    print(\"Epoch %s. Loss: %s, Train_acc %s, Test_acc %s\" % (e, moving_loss, train_accuracy, test_accuracy))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
