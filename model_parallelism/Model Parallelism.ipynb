{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- This is model parallelized version of http://gluon.mxnet.io/chapter05_recurrent-neural-networks/rnns-gluon.html.\n",
    "- Similar to https://mxnet.incubator.apache.org/faq/model_parallel_lstm.html."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import os\n",
    "import time\n",
    "import numpy as np\n",
    "import mxnet as mx\n",
    "from mxnet import gluon, autograd\n",
    "from mxnet.gluon import nn, rnn\n",
    "import collections"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Dictionary(object):\n",
    "    def __init__(self):\n",
    "        self.word2idx = {}\n",
    "        self.idx2word = []\n",
    "\n",
    "    def add_word(self, word):\n",
    "        if word not in self.word2idx:\n",
    "            self.idx2word.append(word)\n",
    "            self.word2idx[word] = len(self.idx2word) - 1\n",
    "        return self.word2idx[word]\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.idx2word)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Corpus(object):\n",
    "    def __init__(self, path):\n",
    "        self.dictionary = Dictionary()\n",
    "        self.train = self.tokenize(path + 'train.txt')\n",
    "        self.valid = self.tokenize(path + 'valid.txt')\n",
    "        self.test = self.tokenize(path + 'test.txt')\n",
    "\n",
    "    def tokenize(self, path):\n",
    "        \"\"\"Tokenizes a text file.\"\"\"\n",
    "        assert os.path.exists(path)\n",
    "        # Add words to the dictionary\n",
    "        with open(path, 'r') as f:\n",
    "            tokens = 0\n",
    "            for line in f:\n",
    "                words = line.split() + ['<eos>']\n",
    "                tokens += len(words)\n",
    "                for word in words:\n",
    "                    self.dictionary.add_word(word)\n",
    "\n",
    "        # Tokenize file content\n",
    "        with open(path, 'r') as f:\n",
    "            ids = np.zeros((tokens,), dtype='int32')\n",
    "            token = 0\n",
    "            for line in f:\n",
    "                words = line.split() + ['<eos>']\n",
    "                for word in words:\n",
    "                    ids[token] = self.dictionary.word2idx[word]\n",
    "                    token += 1\n",
    "\n",
    "        return mx.nd.array(ids, dtype='int32')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`MultiGPULSTM` creates stacked LSTM with layers spread across multiple GPUs. \n",
    "For example, `MultiGPULSTM(0, [1, 2, 2, 1], 400, 200, 0.5)` will create a stacked LSTM with one layer on GPU(0), two layers on GPU(1), two layers on GPU(2), one layer on GPU(3) with a hidden size of 400 embedding size of 200 and dropout probability of .5."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiGPULSTM(object):\n",
    "    \n",
    "    def __init__(self, start_device, num_layers_list, num_hidden, input_size, dropout):\n",
    "        self.lstm_dict = collections.OrderedDict()\n",
    "        device_index = start_device\n",
    "        self.trainers = []\n",
    "        \n",
    "        for num_layers in num_layers_list:\n",
    "            lstm = gluon.rnn.LSTM(num_hidden, num_layers, dropout=dropout, input_size=input_size)\n",
    "            input_size = num_hidden\n",
    "            self.lstm_dict[device_index] = lstm\n",
    "            device_index += 1\n",
    "        \n",
    "    def begin_state(self, *args, **kwargs):\n",
    "        return [lstm.begin_state(ctx=mx.gpu(gpu_num), *args, **kwargs) for gpu_num, lstm in self.lstm_dict.items()]\n",
    "    \n",
    "    def forward(self, inputs, hidden):\n",
    "        hidden_indx = 0\n",
    "\n",
    "        output = inputs\n",
    "        for gpu_num, lstm in self.lstm_dict.items():\n",
    "            next_input = output.as_in_context(mx.gpu(gpu_num))\n",
    "            output, hidden[hidden_indx] = lstm(next_input, hidden[hidden_indx])\n",
    "            hidden_indx += 1\n",
    "        \n",
    "        return output, hidden\n",
    "    \n",
    "    def init_params(self, init=mx.init.Xavier(), force_reinit=False):\n",
    "        for gpu_num, lstm in self.lstm_dict.items():\n",
    "            lstm.collect_params().initialize(init, ctx=mx.gpu(gpu_num), force_reinit=force_reinit)\n",
    "    \n",
    "    def init_trainer(self, optimizer, optimizer_params=None, kvstore='device'):\n",
    "        for gpu_num, lstm in self.lstm_dict.items():\n",
    "            self.trainers.append(gluon.Trainer(lstm.collect_params(), optimizer, optimizer_params, kvstore))\n",
    "\n",
    "    def step(self, batch_size, ignore_stale_grad=False):\n",
    "        for trainer in self.trainers:\n",
    "            trainer.step(batch_size, ignore_stale_grad)\n",
    "\n",
    "    def clip_global_norm(self, max_norm):\n",
    "        for gpu_num, lstm in self.lstm_dict.items():\n",
    "            grads = [i.grad(mx.gpu(gpu_num)) for i in lstm.collect_params().values()]\n",
    "            gluon.utils.clip_global_norm(grads, max_norm)\n",
    "            \n",
    "    def reset_optimizer(self, optimizer, optimizer_params=None):\n",
    "        for trainer in self.trainers:\n",
    "            trainer._init_optimizer(optimizer, optimizer_params)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`LSTMModel` adds an encoder in the beginning and decoder at the end to a `MultiGPULSTM`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LSTMModel():\n",
    "    def __init__(self, vocab_size, embedding_size, num_hidden,\n",
    "                 num_layers_list, dropout=0.5, **kwargs):\n",
    "        self.encoder = nn.Embedding(vocab_size, embedding_size,\n",
    "                                    weight_initializer = mx.init.Uniform(0.1))\n",
    "        self.lstm = MultiGPULSTM(0, num_layers_list, num_hidden, embedding_size, dropout)\n",
    "        self.decoder = nn.Dense(vocab_size, in_units = num_hidden)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.num_hidden = num_hidden\n",
    "        self.num_layers_list = num_layers_list\n",
    "        \n",
    "    def forward(self, inputs, hidden):\n",
    "        embedding = self.encoder(inputs)\n",
    "        embedding = self.dropout(embedding)\n",
    "        output, hidden = self.lstm.forward(embedding, hidden)\n",
    "        output = self.dropout(output)\n",
    "        last_gpu = len(self.num_layers_list) - 1\n",
    "        output = output.as_in_context(mx.gpu(last_gpu))\n",
    "        decoded = self.decoder(output.reshape((-1, self.num_hidden)))\n",
    "        return decoded, hidden\n",
    "    \n",
    "    def begin_state(self, *args, **kwargs):\n",
    "        return self.lstm.begin_state(*args, **kwargs)\n",
    "    \n",
    "    def init_params(self, init=mx.init.Xavier(), force_reinit=False):\n",
    "        self.encoder.collect_params().initialize(init, ctx=mx.gpu(0), force_reinit=force_reinit)\n",
    "        self.lstm.init_params(init, force_reinit)\n",
    "        last_gpu = len(self.num_layers_list) - 1\n",
    "        self.decoder.collect_params().initialize(init, ctx=mx.gpu(last_gpu), force_reinit=force_reinit)\n",
    "    \n",
    "    def init_trainer(self, optimizer, optimizer_params=None, kvstore='device'):\n",
    "        self.encoder_trainer = gluon.Trainer(self.encoder.collect_params(), optimizer, optimizer_params, kvstore)\n",
    "        self.decoder_trainer = gluon.Trainer(self.decoder.collect_params(), optimizer, optimizer_params, kvstore)\n",
    "        self.lstm.init_trainer(optimizer, optimizer_params, kvstore)\n",
    "\n",
    "    def step(self, batch_size, ignore_stale_grad=False):\n",
    "        self.encoder_trainer.step(batch_size, ignore_stale_grad)\n",
    "        self.decoder_trainer.step(batch_size, ignore_stale_grad)\n",
    "        self.lstm.step(batch_size, ignore_stale_grad)\n",
    "\n",
    "    def clip_global_norm(self, max_norm):\n",
    "        self.lstm.clip_global_norm(max_norm)\n",
    "    \n",
    "    def reset_optimizer(self, optimizer, optimizer_params=None):\n",
    "        self.encoder_trainer._init_optimizer(optimizer, optimizer_params)\n",
    "        self.decoder_trainer._init_optimizer(optimizer, optimizer_params)\n",
    "        self.lstm.reset_optimizer(optimizer, optimizer_params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "args_data = 'data/ptb.'\n",
    "args_model = 'lstm'\n",
    "args_emsize = 200\n",
    "args_nhid = 400\n",
    "args_lr = 0.01\n",
    "args_clip = 0.2\n",
    "args_epochs = 1\n",
    "args_batch_size = 32\n",
    "args_bptt = 6\n",
    "args_dropout = 0.2\n",
    "args_log_interval = 100\n",
    "args_save = 'model.param'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus = Corpus(args_data)\n",
    "\n",
    "def batchify(data, batch_size):\n",
    "    \"\"\"Reshape data into (num_example, batch_size)\"\"\"\n",
    "    nbatch = data.shape[0] // batch_size\n",
    "    data = data[:nbatch * batch_size]\n",
    "    data = data.reshape((batch_size, nbatch)).T\n",
    "    return data\n",
    "\n",
    "train_data = batchify(corpus.train, args_batch_size)\n",
    "val_data = batchify(corpus.valid, args_batch_size)\n",
    "test_data = batchify(corpus.test, args_batch_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this example we will create a stacked LSTM with two layers on two GPUs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_layers_list = [1, 1]\n",
    "ctx_begin = mx.gpu(0)\n",
    "ctx_end = mx.gpu(len(num_layers_list) - 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "ntokens = len(corpus.dictionary)\n",
    "\n",
    "model = LSTMModel(ntokens, args_emsize, args_nhid, num_layers_list, args_dropout)\n",
    "model.init_params()\n",
    "model.init_trainer('adadelta', {'learning_rate': args_lr})\n",
    "\n",
    "loss = gluon.loss.SoftmaxCrossEntropyLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_batch(source, i):\n",
    "    seq_len = min(args_bptt, source.shape[0] - 1 - i)\n",
    "    data = source[i : i + seq_len]\n",
    "    target = source[i + 1 : i + 1 + seq_len]\n",
    "    return data, target.reshape((-1,))\n",
    "\n",
    "def detach(hidden):\n",
    "    if isinstance(hidden, (tuple, list)):\n",
    "        hidden = [detach(i) for i in hidden]\n",
    "    else:\n",
    "        hidden = hidden.detach()\n",
    "    return hidden"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def eval(data_source):\n",
    "    total_L = 0.0\n",
    "    ntotal = 0\n",
    "    hidden = model.begin_state(func = mx.nd.zeros, batch_size = args_batch_size)\n",
    "    for i in range(0, data_source.shape[0] - 1, args_bptt):\n",
    "        data, target = get_batch(data_source, i)\n",
    "        data = data.as_in_context(ctx_begin)\n",
    "        target = target.as_in_context(ctx_end)\n",
    "        output, hidden = model.forward(data, hidden)\n",
    "        L = loss(output, target)\n",
    "        total_L += mx.nd.sum(L).asscalar()\n",
    "        ntotal += L.size\n",
    "    return total_L / ntotal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train():\n",
    "    for epoch in range(args_epochs):\n",
    "        total_L = 0.0\n",
    "        start_time = time.time()\n",
    "        hidden = model.begin_state(func = mx.nd.zeros, batch_size = args_batch_size)\n",
    "        for ibatch, i in enumerate(range(0, train_data.shape[0] - 1, args_bptt)):\n",
    "            data, target = get_batch(train_data, i)\n",
    "            data = data.as_in_context(ctx_begin)\n",
    "            target = target.as_in_context(ctx_end)\n",
    "            hidden = detach(hidden)\n",
    "            with autograd.record():\n",
    "                output, hidden = model.forward(data, hidden)\n",
    "                L = loss(output, target)\n",
    "                L.backward()\n",
    "\n",
    "            model.clip_global_norm(args_clip * args_bptt * args_batch_size)\n",
    "\n",
    "            model.step(args_batch_size)\n",
    "            total_L += mx.nd.sum(L).asscalar()\n",
    "\n",
    "            if ibatch % args_log_interval == 0 and ibatch > 0:\n",
    "                cur_L = total_L / args_bptt / args_batch_size / args_log_interval\n",
    "                print('[Epoch %d Batch %d] loss %.2f, perplexity %.2f' % (\n",
    "                    epoch + 1, ibatch, cur_L, math.exp(cur_L)))\n",
    "                total_L = 0.0\n",
    "\n",
    "        val_L = eval(val_data)\n",
    "\n",
    "        print('[Epoch %d] time cost %.2fs, validation loss %.2f, validation perplexity %.2f' % (\n",
    "            epoch + 1, time.time() - start_time, val_L, math.exp(val_L)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 1 Batch 100] loss 7.20, perplexity 1340.10\n",
      "[Epoch 1 Batch 200] loss 6.66, perplexity 781.10\n",
      "[Epoch 1 Batch 300] loss 6.65, perplexity 776.09\n",
      "[Epoch 1 Batch 400] loss 6.55, perplexity 697.08\n",
      "[Epoch 1 Batch 500] loss 6.46, perplexity 640.04\n",
      "[Epoch 1 Batch 600] loss 6.38, perplexity 590.49\n",
      "[Epoch 1 Batch 700] loss 6.36, perplexity 577.39\n",
      "[Epoch 1 Batch 800] loss 6.22, perplexity 501.58\n",
      "[Epoch 1 Batch 900] loss 6.10, perplexity 443.65\n",
      "[Epoch 1 Batch 1000] loss 6.04, perplexity 418.13\n",
      "[Epoch 1 Batch 1100] loss 6.15, perplexity 469.13\n",
      "[Epoch 1 Batch 1200] loss 6.10, perplexity 446.42\n",
      "[Epoch 1 Batch 1300] loss 6.08, perplexity 435.85\n",
      "[Epoch 1 Batch 1400] loss 6.05, perplexity 424.83\n",
      "[Epoch 1 Batch 1500] loss 6.01, perplexity 406.97\n",
      "[Epoch 1 Batch 1600] loss 6.02, perplexity 410.54\n",
      "[Epoch 1 Batch 1700] loss 5.98, perplexity 395.66\n",
      "[Epoch 1 Batch 1800] loss 5.98, perplexity 396.62\n",
      "[Epoch 1 Batch 1900] loss 5.81, perplexity 333.70\n",
      "[Epoch 1 Batch 2000] loss 5.88, perplexity 356.13\n",
      "[Epoch 1 Batch 2100] loss 5.96, perplexity 386.80\n",
      "[Epoch 1 Batch 2200] loss 5.85, perplexity 347.08\n",
      "[Epoch 1 Batch 2300] loss 5.78, perplexity 323.07\n",
      "[Epoch 1 Batch 2400] loss 5.76, perplexity 318.46\n",
      "[Epoch 1 Batch 2500] loss 5.74, perplexity 311.50\n",
      "[Epoch 1 Batch 2600] loss 5.79, perplexity 328.09\n",
      "[Epoch 1 Batch 2700] loss 5.79, perplexity 328.51\n",
      "[Epoch 1 Batch 2800] loss 5.86, perplexity 350.40\n",
      "[Epoch 1 Batch 2900] loss 5.73, perplexity 309.27\n",
      "[Epoch 1 Batch 3000] loss 5.77, perplexity 319.66\n",
      "[Epoch 1 Batch 3100] loss 5.65, perplexity 285.61\n",
      "[Epoch 1 Batch 3200] loss 5.63, perplexity 279.74\n",
      "[Epoch 1 Batch 3300] loss 5.62, perplexity 275.63\n",
      "[Epoch 1 Batch 3400] loss 5.57, perplexity 263.28\n",
      "[Epoch 1 Batch 3500] loss 5.59, perplexity 267.68\n",
      "[Epoch 1 Batch 3600] loss 5.64, perplexity 281.89\n",
      "[Epoch 1 Batch 3700] loss 5.71, perplexity 301.17\n",
      "[Epoch 1 Batch 3800] loss 5.68, perplexity 293.86\n",
      "[Epoch 1 Batch 3900] loss 5.69, perplexity 296.83\n",
      "[Epoch 1 Batch 4000] loss 5.67, perplexity 289.74\n",
      "[Epoch 1 Batch 4100] loss 5.48, perplexity 240.49\n",
      "[Epoch 1 Batch 4200] loss 5.64, perplexity 281.97\n",
      "[Epoch 1 Batch 4300] loss 5.61, perplexity 273.13\n",
      "[Epoch 1 Batch 4400] loss 5.64, perplexity 281.21\n",
      "[Epoch 1 Batch 4500] loss 5.65, perplexity 283.48\n",
      "[Epoch 1 Batch 4600] loss 5.63, perplexity 278.12\n",
      "[Epoch 1 Batch 4700] loss 5.68, perplexity 291.96\n",
      "[Epoch 1 Batch 4800] loss 5.59, perplexity 267.57\n",
      "[Epoch 1] time cost 262.45s, validation loss 5.63, validation perplexity 279.41\n"
     ]
    }
   ],
   "source": [
    "train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
