{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from mxnet import cpu\n",
    "from mxnet.gluon.block import HybridBlock\n",
    "from mxnet.gluon import nn\n",
    "\n",
    "import mxnet as mx\n",
    "import mxnet.ndarray as nd\n",
    "from mxnet import gluon"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AlexNet(HybridBlock):\n",
    "    r\"\"\"AlexNet model from the `\"One weird trick...\" <https://arxiv.org/abs/1404.5997>`_ paper.\n",
    "    Parameters\n",
    "    ----------\n",
    "    classes : int, default 1000\n",
    "        Number of classes for the output layer.\n",
    "    \"\"\"\n",
    "    def __init__(self, classes=1000, **kwargs):\n",
    "        super(AlexNet, self).__init__(**kwargs)\n",
    "        with self.name_scope():\n",
    "            self.features = nn.HybridSequential(prefix='')\n",
    "            with self.features.name_scope():\n",
    "                self.features.add(nn.Conv2D(64, kernel_size=11, strides=4,\n",
    "                                            padding=2, activation='relu'))\n",
    "                self.features.add(nn.MaxPool2D(pool_size=3, strides=2))\n",
    "                self.features.add(nn.Conv2D(192, kernel_size=5, padding=2,\n",
    "                                            activation='relu'))\n",
    "                self.features.add(nn.MaxPool2D(pool_size=3, strides=2))\n",
    "                self.features.add(nn.Conv2D(384, kernel_size=3, padding=1,\n",
    "                                            activation='relu'))\n",
    "                self.features.add(nn.Conv2D(256, kernel_size=3, padding=1,\n",
    "                                            activation='relu'))\n",
    "                self.features.add(nn.Conv2D(256, kernel_size=3, padding=1,\n",
    "                                            activation='relu'))\n",
    "                self.features.add(nn.MaxPool2D(pool_size=3, strides=2))\n",
    "                self.features.add(nn.Flatten())\n",
    "                self.features.add(nn.Dense(4096, activation='relu'))\n",
    "                self.features.add(nn.Dropout(0.5))\n",
    "                self.features.add(nn.Dense(4096, activation='relu'))\n",
    "                self.features.add(nn.Dropout(0.5))\n",
    "\n",
    "            self.output = nn.Dense(classes)\n",
    "\n",
    "    def hybrid_forward(self, F, x):\n",
    "        x = self.features(x)\n",
    "        x = self.output(x)\n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "net = AlexNet()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(net._children)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# class ReversibleConv2D(nn.Conv2D):\n",
    "#     def __init__(self, channels, kernel_size, strides=(1, 1), padding=(0, 0),\n",
    "#                 dilation=(1, 1), groups=1, layout='NCHW',\n",
    "#                 activation=None, use_bias=True, weight_initializer=None,\n",
    "#                 bias_initializer='zeros', in_channels=0, **kwargs):\n",
    "#         super(ReversibleConv2D, self).__init__(channels, kernel_size, strides, padding,\n",
    "#             dilation, groups, layout, activation, use_bias, weight_initializer,\n",
    "#             bias_initializer, in_channels, **kwargs)\n",
    "        \n",
    "#     def hybrid_forward(self, F, x, weight, bias=None):\n",
    "#         this.in_channels = x.shape[1]\n",
    "#         return super(ReversibleConv2D, self).hybrid_forward(self, F, x, weight, bias)\n",
    "    \n",
    "#     def reverse(self, y):\n",
    "# #        print (type(self.weight.data()))\n",
    "#         return nd.Deconvolution(data=y, weight=self.weight.data(), bias=self.bias.data(),\n",
    "#                                 kernel=self.weight.data().shape,\n",
    "#                                 stride=self._kwargs['stride'], dilate=self._kwargs['dilate'],\n",
    "#                                 pad=self._kwargs['pad'], num_filter=self._in_channels, \n",
    "#                                 no_bias=self._kwargs['no_bias'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ReversibleConv2D(gluon.Block):\n",
    "    def __init__(self, channels, kernel_size, strides=(1, 1), padding=(0, 0),\n",
    "                dilation=(1, 1), groups=1, layout='NCHW',\n",
    "                activation=None, use_bias=True, weight_initializer=None,\n",
    "                bias_initializer='zeros', in_channels=0, **kwargs):\n",
    "        super(ReversibleConv2D, self).__init__(**kwargs)\n",
    "        self.conv2d = nn.Conv2D(channels, kernel_size, strides, padding,\n",
    "            dilation, groups, layout, activation, use_bias, weight_initializer,\n",
    "            bias_initializer, in_channels, **kwargs)\n",
    "        #self.conv2d = nn.Conv2D(10, kernel_size=3)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        self.in_channels = x.shape[1]\n",
    "        return self.conv2d(x)\n",
    "    \n",
    "    def reverse(self, y):\n",
    "        conv = self.conv2d\n",
    "        print(conv.weight.data().shape)\n",
    "        print(conv.bias.data().shape)\n",
    "        print(\"kernel:\")\n",
    "        kernel_shape = conv.weight.data().shape[2:]\n",
    "        print(kernel_shape)\n",
    "#         return None\n",
    "        return nd.Deconvolution(data=y, weight=conv.weight.data(), bias=conv.bias.data(),\n",
    "                                kernel=kernel_shape,\n",
    "                                stride=conv._kwargs['stride'], dilate=conv._kwargs['dilate'],\n",
    "                                pad=conv._kwargs['pad'], num_filter=conv._channels, \n",
    "                                no_bias=conv._kwargs['no_bias'])        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_ctx = mx.gpu(0)\n",
    "\n",
    "# net = nn.Sequential()\n",
    "# with net.name_scope():\n",
    "#     net.add(ReversibleConv2D(256, kernel_size=3))\n",
    "\n",
    "x = mx.nd.ones((1, 3, 20, 20))\n",
    "\n",
    "net = ReversibleConv2D(10, kernel_size=3)\n",
    "net.collect_params().initialize(mx.init.Normal(sigma=.01), ctx=model_ctx)\n",
    "\n",
    "y = net(x.as_in_context(model_ctx))\n",
    "\n",
    "print(y.shape)\n",
    "\n",
    "net.reverse(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = (10, 3, 3, 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a[1:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[[[[ 1.29858637  0.57342744 -0.34835348 -0.58361858]\n",
      "   [-2.0417614  -0.1796914  -1.2405448  -0.7325387 ]\n",
      "   [-3.06886029  1.31733298 -1.6384114   0.93694556]\n",
      "   [ 0.592933   -0.50639272 -0.68630707  0.1130666 ]]\n",
      "\n",
      "  [[-0.22556685  0.18525851  1.22298074  0.55320805]\n",
      "   [ 0.25945112 -0.00414529 -0.22740492  0.41514722]\n",
      "   [ 1.91706145 -1.43812132  0.43687493  0.35308638]\n",
      "   [ 0.74126935  2.73936391 -0.65508407  2.11390615]]\n",
      "\n",
      "  [[ 0.54437113 -0.9276405   0.74472493 -0.6918596 ]\n",
      "   [ 0.2654874  -0.11799012  1.21965587  0.18523361]\n",
      "   [ 1.62041867 -0.56247813  0.39254659 -0.9529047 ]\n",
      "   [ 0.05601164 -0.66915834  1.03501856  0.29303032]]]]\n",
      "<NDArray 1x3x4x4 @cpu(0)>\n",
      "(1, 10, 4, 4)\n",
      "(1, 3, 4, 4)\n",
      "\n",
      "[[[[  55.67409134   -7.4365387     7.69010496  -18.32086182]\n",
      "   [-103.55095673   -0.908602    -26.96941376  -44.83856201]\n",
      "   [ -70.33335876  -12.73396397  -19.10709381   20.91474342]\n",
      "   [  30.7226162   -30.73569298  -48.73796463   12.72243881]]\n",
      "\n",
      "  [[ -17.39299774  -11.26985931   44.06104279    9.84715366]\n",
      "   [ -47.21778107   34.12624359  -74.37421417   50.84933472]\n",
      "   [  85.26094818  -93.25151825   71.11813354   10.45162106]\n",
      "   [   8.4536047   109.99107361 -103.89658356   66.42784119]]\n",
      "\n",
      "  [[   5.22134113  -63.75202942   51.36120987  -61.36483383]\n",
      "   [  -5.7288599    12.20728207  101.80821991   22.59580612]\n",
      "   [  78.14104462   11.8103466    -3.17512631  -57.33305359]\n",
      "   [   5.65057707  -52.94085693   47.67569733   47.33943939]]]]\n",
      "<NDArray 1x3x4x4 @cpu(0)>\n"
     ]
    }
   ],
   "source": [
    "input_4x4 = mx.nd.normal(shape=[1, 3, 4, 4])\n",
    "\n",
    "print(input_4x4)\n",
    "\n",
    "kernel_3x3 = mx.nd.normal(shape=[10, 3, 3, 3])\n",
    "conv = mx.nd.Convolution(data=input_4x4, kernel=(3,3), pad=(1,1), weight=kernel_3x3, num_filter=10, no_bias=True)\n",
    "print(conv.shape)\n",
    "\n",
    "#print(conv)\n",
    "\n",
    "transpose = mx.nd.Deconvolution(data=conv, kernel=(3,3), pad=(1,1), weight=kernel_3x3, num_filter=3, no_bias=True)\n",
    "print(transpose.shape)\n",
    "\n",
    "print(transpose)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
