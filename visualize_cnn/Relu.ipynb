{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import mxnet as mx\n",
    "import mxnet.ndarray as nd\n",
    "from mxnet import gluon\n",
    "from mxnet import autograd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ReluOp(mx.operator.CustomOp):\n",
    "    \n",
    "    guided_backprop = True\n",
    "    \n",
    "    def forward(self, is_train, req, in_data, out_data, aux):\n",
    "        x = in_data[0]\n",
    "        y = nd.maximum(x, nd.zeros_like(x))\n",
    "        self.assign(out_data[0], req[0], y)\n",
    "\n",
    "    def backward(self, req, out_grad, in_data, out_data, in_grad, aux):\n",
    "        if ReluOp.guided_backprop:\n",
    "            y = out_data[0]\n",
    "            dy = out_grad[0]\n",
    "            dy_positives = nd.maximum(dy, nd.zeros_like(dy))\n",
    "            y_ones = y.__gt__(0)\n",
    "            dx = dy_positives * y_ones\n",
    "            self.assign(in_grad[0], req[0], dx)\n",
    "        else:\n",
    "            x = in_data[0]\n",
    "            x_gt_zero = x.__gt__(0)\n",
    "            dx = out_grad[0] * x_gt_zero\n",
    "            self.assign(in_grad[0], req[0], dx)\n",
    "\n",
    "@mx.operator.register(\"relu\")\n",
    "class ReluProp(mx.operator.CustomOpProp):\n",
    "    def __init__(self):\n",
    "        super(ReluProp, self).__init__(True)\n",
    "\n",
    "    def infer_shape(self, in_shapes):\n",
    "        data_shape = in_shapes[0]\n",
    "        output_shape = data_shape\n",
    "        return (data_shape,), (output_shape,), ()\n",
    "\n",
    "    def create_operator(self, ctx, in_shapes, in_dtypes):\n",
    "        return ReluOp()  \n",
    "\n",
    "class Activation(mx.gluon.HybridBlock):\n",
    "    @staticmethod\n",
    "    def set_guided_backprop(mode=True):\n",
    "        ReluOp.guided_backprop = mode\n",
    "    \n",
    "    def __init__(self, act_type, **kwargs):\n",
    "        assert act_type == 'relu'\n",
    "        super(Activation, self).__init__(**kwargs)\n",
    "\n",
    "    def hybrid_forward(self, F, x):\n",
    "        return F.Custom(x, op_type='relu')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[1. 2. 0. 0.]\n",
      "<NDArray 4 @cpu(0)>\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "\n",
       "[-1.  2.  0. -0.]\n",
       "<NDArray 4 @cpu(0)>"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "r = Activation('relu')\n",
    "x = mx.nd.array([1, 2, -4, -5])\n",
    "\n",
    "Activation.set_guided_backprop(False)\n",
    "\n",
    "x.attach_grad()\n",
    "with autograd.record():\n",
    "    y = r(x)\n",
    "    print(y)\n",
    "\n",
    "y.backward(nd.array([-1,2,3,-4]))\n",
    "#y.backward(nd.array([1,1,1,1]))\n",
    "\n",
    "x.grad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mx.nd.array([-7]) * mx.nd.array([-0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
