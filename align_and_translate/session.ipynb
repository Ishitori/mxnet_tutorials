{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Attention Mechanism"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import unicode_literals\n",
    "\n",
    "import argparse\n",
    "import random\n",
    "import unicodedata\n",
    "import string\n",
    "import re\n",
    "\n",
    "import numpy as np\n",
    "import mxnet as mx\n",
    "from io import open\n",
    "from mxnet import gluon, autograd, nd\n",
    "from mxnet.gluon import nn, rnn, Block"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_iters = 2200000\n",
    "lr = 0.01\n",
    "max_length = 10\n",
    "num_layers = 1\n",
    "hidden_size = 256\n",
    "teacher_forcing_ratio = 0.5\n",
    "log_interval = 500\n",
    "test = True\n",
    "\n",
    "SOS_token = 0\n",
    "EOS_token = 1\n",
    "\n",
    "ctx = mx.gpu(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Lang:\n",
    "    def __init__(self, name):\n",
    "        self.name = name\n",
    "        self.word2index = {}\n",
    "        self.word2count = {}\n",
    "        self.index2word = {0: \"SOS\", 1: \"EOS\"}\n",
    "        self.n_words = 2  # Count SOS and EOS\n",
    "\n",
    "    def addSentence(self, sentence):\n",
    "        for word in sentence.split(' '):\n",
    "            self.addWord(word)\n",
    "\n",
    "    def addWord(self, word):\n",
    "        if word not in self.word2index:\n",
    "            self.word2index[word] = self.n_words\n",
    "            self.word2count[word] = 1\n",
    "            self.index2word[self.n_words] = word\n",
    "            self.n_words += 1\n",
    "        else:\n",
    "            self.word2count[word] += 1\n",
    "\n",
    "def unicodeToAscii(s):\n",
    "    return ''.join(\n",
    "        c for c in unicodedata.normalize('NFD', s)\n",
    "        if unicodedata.category(c) != 'Mn'\n",
    "    )\n",
    "\n",
    "\n",
    "# Lowercase, trim, and remove non-letter characters\n",
    "def normalizeString(s):\n",
    "    s = unicodeToAscii(s.lower().strip())\n",
    "    s = re.sub(r\"([.!?])\", r\" \\1\", s)\n",
    "    s = re.sub(r\"[^a-zA-Z.!?]+\", r\" \", s)\n",
    "    return s\n",
    "\n",
    "def readLangs(lang1, lang2, reverse=False):\n",
    "    print(\"Reading lines...\")\n",
    "\n",
    "    # Read the file and split into lines\n",
    "    lines = open('%s-%s.txt' % (lang1, lang2), encoding='utf-8').\\\n",
    "        read().strip().split('\\n')\n",
    "\n",
    "    # Split every line into pairs and normalize\n",
    "    pairs = [[normalizeString(s) for s in l.split('\\t')] for l in lines]\n",
    "\n",
    "    # Reverse pairs, make Lang instances\n",
    "    if reverse:\n",
    "        pairs = [list(reversed(p)) for p in pairs]\n",
    "        input_lang = Lang(lang2)\n",
    "        output_lang = Lang(lang1)\n",
    "    else:\n",
    "        input_lang = Lang(lang1)\n",
    "        output_lang = Lang(lang2)\n",
    "\n",
    "    return input_lang, output_lang, pairs\n",
    "\n",
    "eng_prefixes = (\n",
    "    \"i am \", \"i m \",\n",
    "    \"he is\", \"he s \",\n",
    "    \"she is\", \"she s\",\n",
    "    \"you are\", \"you re \",\n",
    "    \"we are\", \"we re \",\n",
    "    \"they are\", \"they re \"\n",
    ")\n",
    "\n",
    "def filterPair(p, max_length):\n",
    "    return len(p[0].split(' ')) < max_length and \\\n",
    "        len(p[1].split(' ')) < max_length and \\\n",
    "        p[1].startswith(eng_prefixes)\n",
    "\n",
    "def filterPairs(pairs, max_length):\n",
    "    return [pair for pair in pairs if filterPair(pair, max_length)]\n",
    "\n",
    "def prepareData(lang1, lang2, max_length, reverse=False):\n",
    "    input_lang, output_lang, pairs = readLangs(lang1, lang2, reverse)\n",
    "    print(\"Read %s sentence pairs\" % len(pairs))\n",
    "    pairs = filterPairs(pairs, max_length)\n",
    "    print(\"Trimmed to %s sentence pairs\" % len(pairs))\n",
    "    print(\"Counting words...\")\n",
    "    for pair in pairs:\n",
    "        input_lang.addSentence(pair[0])\n",
    "        output_lang.addSentence(pair[1])\n",
    "    print(\"Counted words:\")\n",
    "    print(input_lang.name, input_lang.n_words)\n",
    "    print(output_lang.name, output_lang.n_words)\n",
    "    return input_lang, output_lang, pairs\n",
    "    \n",
    "def indexesFromSentence(lang, sentence):\n",
    "    return [lang.word2index[word] for word in sentence.split(' ')]\n",
    "\n",
    "def variableFromSentence(lang, sentence):\n",
    "    indexes = indexesFromSentence(lang, sentence)\n",
    "    indexes.append(EOS_token)\n",
    "    result = nd.array(indexes)\n",
    "    return result\n",
    "\n",
    "def variablesFromPair(pair):\n",
    "    input_variable = variableFromSentence(input_lang, pair[0])\n",
    "    target_variable = variableFromSentence(output_lang, pair[1])\n",
    "    return (input_variable, target_variable)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_lang, output_lang, pairs = prepareData('eng', 'fra', max_length, True)\n",
    "print(random.choice(pairs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pairs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pair = random.choice(pairs)\n",
    "pair"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_pairs = variablesFromPair(pair)\n",
    "training_pairs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EncoderRNN(Block):\n",
    "    def __init__(self, input_size, hidden_size, n_layers):\n",
    "        super(EncoderRNN, self).__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "        self.n_layers = n_layers\n",
    "\n",
    "        with self.name_scope():\n",
    "            self.embedding = nn.Embedding(input_size, hidden_size)\n",
    "            self.gru = rnn.GRU(hidden_size, input_size=self.hidden_size)\n",
    "\n",
    "    def forward(self, input, hidden):\n",
    "        output = self.embedding(input).swapaxes(0, 1)\n",
    "        for i in range(self.n_layers):\n",
    "            output, hidden = self.gru(output, hidden)\n",
    "        return output, hidden\n",
    "\n",
    "    def initHidden(self, ctx):\n",
    "        return [nd.zeros((1, 1, self.hidden_size), ctx=ctx)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AttnDecoderRNN(Block):\n",
    "    def __init__(self, hidden_size, output_size, n_layers, max_length, dropout_p=0.1):\n",
    "        super(AttnDecoderRNN, self).__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "        self.output_size = output_size\n",
    "        self.n_layers = n_layers\n",
    "        self.dropout_p = dropout_p\n",
    "        self.max_length = max_length\n",
    "\n",
    "        with self.name_scope():\n",
    "            self.embedding = nn.Embedding(self.output_size, self.hidden_size)\n",
    "            self.attn = nn.Dense(self.max_length, in_units=self.hidden_size)\n",
    "            self.attn_combine = nn.Dense(self.hidden_size, in_units=self.hidden_size * 2)\n",
    "            if self.dropout_p > 0:\n",
    "                self.dropout = nn.Dropout(self.dropout_p)\n",
    "            self.gru = rnn.GRU(self.hidden_size, input_size=self.hidden_size)\n",
    "            self.out = nn.Dense(self.output_size, in_units=self.hidden_size)\n",
    "\n",
    "    def forward(self, input, hidden, encoder_outputs):\n",
    "        \n",
    "        # Input shape is 1. Just a scalar. Examples of input: [0], [213], [23], [76]\n",
    "\n",
    "        # Hidden is a list because LSTMs could be stacked.\n",
    "        # Hidden[0] is the hidden state of the first LSTM\n",
    "        # Shape of Hidden[0] is (1, 1, 256)\n",
    "\n",
    "        # encoder_outputs is a (10, 256) tensor\n",
    "        \n",
    "        # embedded is a (1, 256) tensor\n",
    "        embedded = self.embedding(input)\n",
    "\n",
    "        # Dropout for regularization\n",
    "        if self.dropout_p > 0:\n",
    "            embedded = self.dropout(embedded)\n",
    "\n",
    "        output = embedded.expand_dims(0)\n",
    "        for i in range(self.n_layers):\n",
    "            output, hidden = self.gru(output, hidden)\n",
    "            output = nd.relu(output)\n",
    "        \n",
    "        # Depending on the hidden state, predict attention weights.\n",
    "        # self.attn is a dense layer.\n",
    "        # attn_weights is a (1, 10) vector - one scalar for each time step in the input.\n",
    "        attn_weights = nd.softmax(self.attn(hidden[0].flatten()))\n",
    "        \n",
    "        # attn_weights is (1, 10)\n",
    "        # encoder_outputs is (10, 256)\n",
    "        # attn_applied is (1, 1, 256)\n",
    "        # attn_applied is ct in the paper\n",
    "        attn_applied = nd.batch_dot(attn_weights.expand_dims(0),\n",
    "                                 encoder_outputs.expand_dims(0))\n",
    "        \n",
    "        out = nd.concat(attn_applied.flatten(), hidden[0].flatten(), dim=1)\n",
    "        out = self.attn_combine(out)\n",
    "        out = nd.tanh(out)\n",
    "        \n",
    "        out = self.out(out)\n",
    "        return out, hidden, attn_weights\n",
    "\n",
    "    def initHidden(self, ctx):\n",
    "        return [nd.zeros((1, 1, self.hidden_size), ctx=ctx)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(input_variable, target_variable, encoder, decoder, teacher_forcing_ratio,\n",
    "          encoder_optimizer, decoder_optimizer, criterion, max_length, ctx):\n",
    "    with autograd.record():\n",
    "        loss = nd.zeros((1,), ctx=ctx)\n",
    "\n",
    "        encoder_hidden = encoder.initHidden(ctx)\n",
    "\n",
    "        input_length = input_variable.shape[0]\n",
    "        target_length = target_variable.shape[0]\n",
    "\n",
    "        encoder_outputs, encoder_hidden = encoder(\n",
    "                input_variable.expand_dims(0), encoder_hidden)\n",
    "\n",
    "        if input_length < max_length:\n",
    "            encoder_outputs = nd.concat(encoder_outputs.flatten(),\n",
    "                nd.zeros((max_length - input_length, encoder.hidden_size), ctx=ctx), dim=0)\n",
    "        else:\n",
    "            encoder_outputs = encoder_outputs.flatten()\n",
    "\n",
    "\n",
    "        decoder_input = nd.array([SOS_token], ctx=ctx)\n",
    "\n",
    "        decoder_hidden = encoder_hidden\n",
    "\n",
    "        use_teacher_forcing = True if random.random() < teacher_forcing_ratio else False\n",
    "\n",
    "        if use_teacher_forcing:\n",
    "            # Teacher forcing: Feed the target as the next input\n",
    "            for di in range(target_length):\n",
    "                decoder_output, decoder_hidden, decoder_attention = decoder(\n",
    "                    decoder_input, decoder_hidden, encoder_outputs)\n",
    "\n",
    "                loss = nd.add(loss, criterion(decoder_output, target_variable[di]))\n",
    "                #print criterion(decoder_output, target_variable[di])\n",
    "                decoder_input = target_variable[di]  # Teacher forcing\n",
    "\n",
    "        else:\n",
    "            # Without teacher forcing: use its own predictions as the next input\n",
    "            for di in range(target_length):\n",
    "                decoder_output, decoder_hidden, decoder_attention = decoder(\n",
    "                    decoder_input, decoder_hidden, encoder_outputs)\n",
    "                topi = decoder_output.argmax(axis=1)\n",
    "\n",
    "                decoder_input = nd.array([topi.asscalar()], ctx=ctx)\n",
    "\n",
    "                loss = nd.add(loss, criterion(decoder_output, target_variable[di]))\n",
    "\n",
    "                if topi.asscalar() == EOS_token:\n",
    "                    break\n",
    "\n",
    "        loss.backward()\n",
    "\n",
    "    encoder_optimizer.step(1)\n",
    "    decoder_optimizer.step(1)\n",
    "\n",
    "    return loss.asscalar()/target_length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(encoder, decoder, sentence, max_length=max_length):\n",
    "    input_tensor = variableFromSentence(input_lang, sentence).as_in_context(ctx)\n",
    "    input_length = input_tensor.shape[0]\n",
    "    print(input_length)\n",
    "    \n",
    "    encoder_hidden = encoder.initHidden(ctx)\n",
    "\n",
    "\n",
    "    encoder_outputs = nd.zeros((max_length, encoder.hidden_size), ctx=ctx)\n",
    "\n",
    "\n",
    "    for ei in range(input_length):\n",
    "        \n",
    "        encoder_output, encoder_hidden = encoder(input_tensor[ei].expand_dims(0),\n",
    "                                                 encoder_hidden)\n",
    "        encoder_outputs[ei] += encoder_output[0, 0]\n",
    "        \n",
    "    decoder_input = nd.array([SOS_token], ctx=ctx)\n",
    "\n",
    "    decoder_hidden = encoder_hidden\n",
    "\n",
    "    decoded_words = []\n",
    "    decoder_attentions = nd.zeros((max_length, max_length), ctx=ctx)\n",
    "\n",
    "    for di in range(max_length):\n",
    "        decoder_output, decoder_hidden, decoder_attention = decoder(\n",
    "            decoder_input, decoder_hidden, encoder_outputs)\n",
    "        decoder_attentions[di] = decoder_attention[0]\n",
    "        \n",
    "        topi = nd.argmax(decoder_output, axis=1)\n",
    "        \n",
    "        if topi.asscalar() == EOS_token:\n",
    "            decoded_words.append('<EOS>')\n",
    "            break\n",
    "        else:\n",
    "            decoded_words.append(output_lang.index2word[topi.asscalar()])\n",
    "\n",
    "        decoder_input = topi\n",
    "\n",
    "    return decoded_words, decoder_attentions[:di + 1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def trainIters(encoder, decoder, ctx):\n",
    "    print_every = log_interval\n",
    "    print_loss_total = 0  # Reset every print_every\n",
    "\n",
    "    encoder.initialize(ctx=ctx)\n",
    "    decoder.initialize(ctx=ctx)\n",
    "\n",
    "    encoder_optimizer = gluon.Trainer(encoder.collect_params(), 'sgd', {'learning_rate': lr})\n",
    "    decoder_optimizer = gluon.Trainer(decoder.collect_params(), 'sgd', {'learning_rate': lr})\n",
    "\n",
    "    training_pairs = [variablesFromPair(random.choice(pairs))\n",
    "                      for i in range(num_iters)]\n",
    "\n",
    "    criterion = gluon.loss.SoftmaxCrossEntropyLoss()\n",
    "\n",
    "    for iter in range(1, num_iters + 1):\n",
    "        training_pair = training_pairs[iter - 1]\n",
    "        input_variable = training_pair[0].as_in_context(ctx)\n",
    "        target_variable = training_pair[1].as_in_context(ctx)\n",
    "\n",
    "        loss = train(input_variable, target_variable, encoder, decoder, teacher_forcing_ratio,\n",
    "                     encoder_optimizer, decoder_optimizer, criterion, max_length, ctx)\n",
    "\n",
    "        print_loss_total += loss\n",
    "\n",
    "        if iter % print_every == 0:\n",
    "            print_loss_avg = print_loss_total / print_every\n",
    "            print_loss_total = 0\n",
    "            print(\"Iter: %d, Loss: %f\" % (iter, print_loss_avg))\n",
    "            \n",
    "            \n",
    "            pair = random.choice(pairs)\n",
    "            print(pair)\n",
    "            print(evaluate(encoder, attn_decoder, pair[0], 10))\n",
    "            \n",
    "            encoder.save_params(\"encoder_%d.params\" % iter)\n",
    "            attn_decoder.save_params(\"decoder_%d.params\" % iter)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoder = EncoderRNN(input_lang.n_words, hidden_size, num_layers)\n",
    "attn_decoder = AttnDecoderRNN(hidden_size, output_lang.n_words,\n",
    "                               num_layers, max_length, dropout_p=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainIters(encoder, attn_decoder, ctx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pair = random.choice(pairs)\n",
    "print(pair)\n",
    "evaluate(encoder, attn_decoder, pair[0], 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "#plt.switch_backend('agg')\n",
    "import matplotlib.ticker as ticker\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "def showPlot(points):\n",
    "    plt.figure()\n",
    "    fig, ax = plt.subplots()\n",
    "    # this locator puts ticks at regular intervals\n",
    "    loc = ticker.MultipleLocator(base=0.2)\n",
    "    ax.yaxis.set_major_locator(loc)\n",
    "    plt.plot(points)\n",
    "\n",
    "\n",
    "def showAttention(input_sentence, output_words, attentions):\n",
    "    # Set up figure with colorbar\n",
    "    fig = plt.figure()\n",
    "    ax = fig.add_subplot(111)\n",
    "    cax = ax.matshow(attentions.asnumpy(), cmap='bone')\n",
    "    fig.colorbar(cax)\n",
    "\n",
    "    # Set up axes\n",
    "    ax.set_xticklabels([''] + input_sentence.split(' ') +\n",
    "                       ['<EOS>'], rotation=90)\n",
    "    ax.set_yticklabels([''] + output_words)\n",
    "\n",
    "    # Show label at every tick\n",
    "    ax.xaxis.set_major_locator(ticker.MultipleLocator(1))\n",
    "    ax.yaxis.set_major_locator(ticker.MultipleLocator(1))\n",
    "\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "def evaluateAndShowAttention(input_sentence):\n",
    "    output_words, attentions = evaluate(\n",
    "        encoder, attn_decoder, input_sentence)\n",
    "    print('input =', input_sentence)\n",
    "    print('output =', ' '.join(output_words))\n",
    "    showAttention(input_sentence, output_words, attentions)\n",
    "\n",
    "\n",
    "evaluateAndShowAttention(\"elle a cinq ans de moins que moi .\")\n",
    "\n",
    "evaluateAndShowAttention(\"elle est trop petit .\")\n",
    "\n",
    "evaluateAndShowAttention(\"je ne crains pas de mourir .\")\n",
    "\n",
    "evaluateAndShowAttention(\"c est un jeune directeur plein de talent .\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:mx]",
   "language": "python",
   "name": "conda-env-mx-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
